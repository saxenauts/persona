# Persona Memory System - Evaluation Framework

A comprehensive evaluation framework for testing long-term memory systems against academic benchmarks, featuring deep observability, reproducible sampling, and multi-system comparison.

[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Unified Benchmark Architecture](#unified-benchmark-architecture)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Results (Placeholders)](#results)
- [Usage Guide](#usage-guide)
- [Architecture](#architecture)
- [Configuration](#configuration)
- [Extending the Framework](#extending-the-framework)
- [Project Structure](#project-structure)
- [Contributing](#contributing)

---

## Overview

This evaluation framework tests memory systems on their ability to:
- **Recall user-stated facts** across conversations
- **Track temporal information** (dates, durations, ordering)
- **Aggregate multi-session data** (counting, comparison)
- **Update knowledge** when information changes
- **Personalize responses** based on preferences
- **Know when to abstain** (say "I don't know")

The framework supports multiple benchmarks (LongMemEval, PersonaMem) and memory systems (Persona, Mem0, Zep/Graphiti), with deep logging for debugging and analysis.

---

## Features

âœ… **Dual Benchmark Support**
- [LongMemEval](https://github.com/xiaowu0162/LongMemEval) (ICLR 2025) - 500 questions, 7 question types
- [PersonaMem](https://github.com/bowen-upenn/PersonaMem) (COLM 2025) - 589+ questions, 7 personalization skills

âœ… **Stratified Sampling**
- Reproducible golden sets (695 curated questions)
- Balanced coverage across question types
- Fixed random seeds for reproducibility

âœ… **Deep Observability**
- Structured JSONL logging (ingestion â†’ retrieval â†’ generation â†’ evaluation)
- Retrieval quality metrics (vector search, graph traversal)
- Timing breakdowns per pipeline stage
- Failure pattern detection

âœ… **Multi-System Comparison**
- Pluggable adapter architecture
- Parallel evaluation support
- Side-by-side performance analysis

âœ… **Production-Ready CLI**
- Single-command evaluation runs
- Interactive result analysis
- Automated report generation

---

## Unified Benchmark Architecture

We combine two benchmarks to test both **reasoning** and **recall**:

| Benchmark | What it Tests | Reference |
|-----------|---------------|----------|
| [LongMemEval](https://github.com/xiaowu0162/LongMemEval) | Temporal logic, multi-session aggregation | ICLR 2025 |
| [PersonaMem](https://github.com/bowen-upenn/PersonaMem) | Factual precision, personalization | COLM 2025 |

### Evaluation Approach

We report **two separate benchmark scores** (paper-aligned):
- **LongMemEval** macro-by-qtype accuracy
- **PersonaMem** macro-by-qtype accuracy

**Scoring methodology:**
- LongMemEval: LLM judge (configurable via `EVAL_JUDGE_MODEL`, default `gpt-5-mini`)
- PersonaMem: Exact match on multiple choice
- Macro-by-type: Each question type weighted equally (skill fairness)

**Test set options:**
| Set | Purpose | Size |
|-----|---------|------|
| `--samples 2` | Smoke test | ~12 questions |
| `--samples 10` | Quick test | ~60 questions |
| `--golden-set` | Full eval | ~695 questions |

**Unified golden set config:**
- `evals/configs/golden_set.yaml` defines the combined sampling plan across LongMemEval + PersonaMem.
- `evals/scripts/generate_golden_sets.py` writes per-benchmark golden sets plus `combined_golden_set_manifest.json` for cross-system comparisons.

**Full dataset config (paper-complete):**
- `evals/configs/full_dataset.yaml` runs all questions (~1089 total).


### Capability Coverage

**Currently Tested:**

| Category | Capability | Source |
|----------|------------|--------|
| Recall | Static facts (names, events) | PersonaMem |
| Temporal | Date ordering, durations | LongMemEval |
| Logic | Aggregation across sessions | LongMemEval |
| Updates | Correcting outdated info | LongMemEval |
| Abstention | Knowing when to say "I don't know" | LongMemEval |
| Alignment | Preference-based suggestions | PersonaMem |

**Persona-Specific (Future Tests):**

| Category | Capability | Why Persona Excels |
|----------|------------|-------------------|
| Goal Tracking | "What are my current tasks?" | Explicit `Goal` nodes |
| Inventory | Tracking items (food, ingredients) | Goal nodes with state |
| Proactivity | "Remind me about X" | Temporal + Goal linking |
| Multi-Hop | "Did I like X before Y changed?" | Graph traversal |
| Narrative | Understanding life "stories" | Episode temporal chaining |

---

## Installation

### Prerequisites

- Python 3.12+
- Poetry (package manager)
- Docker (for Neo4j graph database)

### Setup

```bash
# Clone the repository
git clone https://github.com/saxenauts/persona.git
cd persona

# Install dependencies
poetry install

# Download benchmark datasets
poetry run python evals/scripts/download_personamem.py

# Verify LongMemEval dataset
poetry run python evals/scripts/verify_longmemeval_oracle.py

# Generate golden sets (695 stratified questions)
# Uses evals/configs/golden_set.yaml and writes combined_golden_set_manifest.json
poetry run python evals/scripts/generate_golden_sets.py

# Create default config files
poetry run python -m evals.cli create-configs
```

### Environment Variables

```bash
# OpenAI (default)
export OPENAI_API_KEY="your-openai-key"
export LLM_SERVICE="openai/gpt-4o-mini"
export EMBEDDING_SERVICE="openai/text-embedding-3-small"

# Eval Judge Model (optional, default: gpt-5-mini)
export EVAL_JUDGE_MODEL="gpt-5-mini"

# Parallel ingestion (optional, default: 5)
export INGEST_SESSION_CONCURRENCY="5"

# Azure OpenAI (optional)
export AZURE_API_KEY="your-key"
export AZURE_API_BASE="https://your-endpoint.openai.azure.com/"
export AZURE_CHAT_DEPLOYMENT="gpt-5.2"
export AZURE_EMBEDDING_DEPLOYMENT="text-embedding-3-small"
export GRAPHITI_PROVIDER="azure"

# Neo4j (for Persona adapter)
export NEO4J_URI="bolt://localhost:7687"
export NEO4J_USERNAME="neo4j"
export NEO4J_PASSWORD="your-password"
```

---

## Quick Start

### Run Your First Evaluation

```bash
# Quick test (sample 5 per type, LongMemEval subset)
poetry run python -m evals.cli run \
  --benchmark longmemeval \
  --samples 5 \
  --seed 42

# Use pre-generated golden set (~695 questions)
poetry run python -m evals.cli run \
  --config evals/configs/full_eval.yaml \
  --golden-set

# Analyze results
poetry run python -m evals.cli analyze run_20241221_143052 --summary
```

---

## Results

> [!NOTE]
> **Status (COMPLETE)**: LongMemEval evaluation complete for both systems.
>
> ðŸ“Š **Latest:** LongMemEval full results (Dec 25, 2025)
> ðŸŽ¯ **Next:** PersonaMem evaluation
> ðŸ“ **Analysis:** See `evals/analysis/CURRENT_STATUS.md` for detailed breakdown

### LongMemEval Performance (Dec 25, 2025)

**Benchmark:** 500 total questions (full dataset)

| System | Overall | Coverage | Single-Session | Multi-Session | Temporal | Knowledge Update |
|--------|---------|----------|----------------|---------------|----------|------------------|
| **Persona** | **64.1%** | 220/500 (44%) | 85.7% | **68.3%** | **36.7%** | **75.0%** |
| Graphiti (Merged) | 53.2% | 500/500 (100%) | **91.9%** | 29.6% | 22.5% | 59.8% |

**Winner: Persona (+10.9% overall accuracy)**

#### Detailed Breakdown by Question Type

| Question Type | Persona Acc | Persona N | Graphiti Acc | Graphiti N | Winner |
|--------------|-------------|-----------|--------------|------------|--------|
| single-session-user | 85.7% (30/35) | 35 | **91.9%** (68/74) | 74 | Graphiti |
| single-session-assistant | N/A | 0 | 82.5% (47/57) | 57 | - |
| single-session-preference | **72.0%** (18/25) | 25 | 45.7% (16/35) | 35 | Persona |
| multi-session | **68.3%** (41/60) | 60 | 29.6% (60/203) | 203 | Persona |
| temporal-reasoning | **36.7%** (22/60) | 60 | 22.5% (38/169) | 169 | Persona |
| knowledge-update | **75.0%** (30/40) | 40 | 59.8% (52/87) | 87 | Persona |

**Key Findings:**
- âœ… **Persona wins overall** (64.1% vs 53.2%, +10.9%)
- âœ… **Persona excels at multi-session aggregation** (68% vs 30%)
- âœ… **Persona better at temporal reasoning** (37% vs 23%)
- âœ… **Graphiti faster retrieval** (1.2s vs 12.8s avg) but much slower ingestion
- âš ï¸ **Both struggle with temporal reasoning** (37% vs 23%) - architectural gap

### PersonaMem Performance

> [!CAUTION]
> **Status:** Not yet evaluated. Scheduled for next run.

Expected coverage: 369 questions (full PersonaMem 32k variant)

### Latency Comparison

| System | Phase | Avg Time | Notes |
|--------|-------|----------|-------|
| **Persona** | Retrieval | 12.8s | Graph traversal + ranking |
| | **Total/question** | ~13s | Retrieval-dominated |
| **Graphiti** | Ingestion | 15.8m (hard cases) | **Bottleneck** - 45+ LLM calls/session |
| | Retrieval | 1.2s | **10x faster** than Persona |
| | Generation | 3.5s | Standard LLM latency |
| | **Total/question** | 16m+ (hard) / 3.4m (easy) | Ingestion-dominated |

**Production Implications:**
- Persona: Fast one-time ingestion, slower queries (good for write-once-read-many)
- Graphiti: Very slow ingestion on complex sessions, fast queries (bottleneck for real-time updates)

### Completion Status

**Completed:**
- âœ… Persona LongMemEval partial (220/500 questions) - **64.1%**
- âœ… Graphiti LongMemEval COMPLETE (500/500 questions) - **53.2%**

**Pending:**
- ðŸ”„ Persona full golden set (695 questions = 326 LongMemEval + 369 PersonaMem)
- ðŸ”„ Graphiti PersonaMem (369 questions)

**Analysis Files:**
- `analysis/CURRENT_STATUS.md` - Current standings and comparison
- `analysis/GRAPHITI_ANALYSIS_REPORT.md` - Comprehensive comparison
- `results/retry_20251225_011451/run_20251225_015400/summary.json` - Retry run results

### Key Findings

ðŸ“Š **Strengths**:
- Multi-session aggregation: Persona 68%, Graphiti 30%
- Knowledge updates: Persona 75%, Graphiti 60%
- Temporal reasoning: Persona 37%, Graphiti 23%

ðŸ“‰ **Challenges**:
- Both struggle with temporal reasoning (<40%)
- Graphiti ingestion too slow for production (16m+ on hard cases)
- Persona retrieval could be faster (12.8s vs 1.2s)

ðŸ” **System Comparison**:
- **Persona**: Best at multi-session, knowledge updates, and temporal. +11% overall.
- **Graphiti**: Fastest retrieval, best single-session recall. Bottlenecked by ingestion.

---

## Usage Guide

### Running Evaluations

#### Option 1: Using Config Files (Recommended)

```bash
# Full evaluation (both benchmarks, ~695 questions)
poetry run python -m evals.cli run --config evals/configs/full_eval.yaml

# Full dataset run (paper-complete, ~1089 questions)
poetry run python -m evals.cli run --config evals/configs/full_dataset.yaml

# Quick test (small sample)
poetry run python -m evals.cli run --config evals/configs/quick_test.yaml

# LongMemEval only
poetry run python -m evals.cli run --config evals/configs/longmemeval_only.yaml
```

#### Option 2: Command-Line Arguments

```bash
# Custom sampling
poetry run python -m evals.cli run \
  --benchmark longmemeval \
  --types multi-session,temporal-reasoning \
  --samples 20 \
  --seed 42 \
  --workers 5

# Multiple benchmarks
poetry run python -m evals.cli run \
  --benchmark longmemeval \
  --benchmark personamem \
  --samples 10

# Compare multiple adapters
poetry run python -m evals.cli run \
  --benchmark longmemeval \
  --adapter persona \
  --adapter mem0 \
  --samples 15
```

#### Option 3: Use Pre-Generated Golden Sets

```bash
# Use golden sets instead of random sampling
poetry run python -m evals.cli run \
  --config evals/configs/full_eval.yaml \
  --golden-set
```

### Analyzing Results

```bash
# Summary report
poetry run python -m evals.cli analyze run_20241221_143052 --summary

# Filter by question type
poetry run python -m evals.cli analyze run_20241221_143052 \
  --type multi-session

# Show only failures
poetry run python -m evals.cli analyze run_20241221_143052 \
  --type multi-session \
  --failures

# Retrieval quality analysis
poetry run python -m evals.cli analyze run_20241221_143052 --retrieval
```

### Comparing Runs

```bash
# Compare two runs
poetry run python -m evals.cli compare run_a run_b

# Compare three systems
poetry run python -m evals.cli compare \
  persona_20241221 \
  mem0_20241221 \
  zep_20241221
```

For per-type latency/token comparisons using the unified golden set:

```bash
poetry run python evals/scripts/compare_runs.py \
  --runs run_a,run_b \
  --manifest evals/data/golden_sets/combined_golden_set_manifest.json
```

Aggregate timing metrics across runs:

```bash
poetry run python -m evals.cli aggregate --runs run_a,run_b,run_c
```

Judge deferred LongMemEval runs:

```bash
poetry run python -m evals.cli judge run_20241221_143052
```

---

## Architecture

### Pipeline Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Data Loading                         â”‚
â”‚  - LongMemEval Oracle (500 questions)                   â”‚
â”‚  - PersonaMem 32k (589 questions)                       â”‚
â”‚  - Stratified sampling â†’ Golden sets (695 questions)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Memory System Adapter                   â”‚
â”‚  - Ingest conversation history                          â”‚
â”‚  - Query with test question                             â”‚
â”‚  - Log retrieval stats (vector + graph)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Answer Generation (LLM)                    â”‚
â”‚  - Context + question â†’ generated answer                â”‚
â”‚  - Track tokens, latency, model used                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Evaluation (Judge)                     â”‚
â”‚  - LongMemEval: Binary (gpt-5.2 judge)                   â”‚
â”‚  - PersonaMem: Exact match (a/b/c/d)                    â”‚
â”‚  - Task-specific prompts                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             Deep Logging & Analysis                     â”‚
â”‚  - JSONL logs (queryable)                               â”‚
â”‚  - Failure pattern detection                            â”‚
â”‚  - Retrieval quality metrics                            â”‚
â”‚  - Comparative reports                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Adapter Interface

All memory systems implement the `MemorySystem` base class:

```python
from evals.adapters.base import MemorySystem

class YourAdapter(MemorySystem):
    def add_session(self, user_id: str, session_data: str, date: str) -> None:
        """Ingest a single session into the memory system."""
        pass
    
    def add_sessions(self, user_id: str, sessions: List[Dict]) -> None:
        """Bulk ingest sessions. Each dict has 'content' and 'date' keys."""
        pass

    def query(self, user_id: str, question: str) -> str:
        """Query memory and generate answer."""
        pass

    def reset(self, user_id: str) -> None:
        """Clear memory for a user."""
        pass
```

### Deep Logging Schema

Each question evaluation produces a structured log:

```json
{
  "question_id": "gpt4_abc123",
  "timestamp": "2024-12-21T14:30:52Z",
  "user_id": "user_123",
  "benchmark": "longmemeval",
  "question_type": "multi-session",
  "question": "How many times did I visit the gym?",

  "ingestion": {
    "duration_ms": 15420,
    "sessions_count": 47,
    "memories_created": {"episodes": 52, "psyche": 18, "goals": 7},
    "nodes_created": 77,
    "relationships_created": 134,
    "embeddings_generated": 77
  },

  "retrieval": {
    "query": "How many times did I visit the gym?",
    "duration_ms": 1847,
    "vector_search": {
      "top_k": 5,
      "seeds": [
        {"node_id": "episode_42", "score": 0.94, "type": "episode"}
      ]
    },
    "graph_traversal": {
      "max_hops": 2,
      "nodes_visited": 23,
      "final_ranked_nodes": ["episode_42", "episode_38"]
    },
    "context_size_tokens": 3452
  },

  "generation": {
    "duration_ms": 2310,
    "model": "gpt-4o-mini",
    "answer": "You visited the gym 3 times."
  },

  "evaluation": {
    "gold_answer": "3",
    "correct": true,
    "judge_model": "gpt-4o"
  }
}
```

---

## Configuration

### Config File Format (`configs/full_eval.yaml`)

```yaml
longmemeval:
  source: evals/data/longmemeval_oracle.json
  sample_sizes:
    single-session-user: 35
    multi-session: 60
    temporal-reasoning: 60
    knowledge-update: 40
    single-session-preference: 25

personamem:
  source: evals/data/personamem
  variant: 32k
  sample_sizes:
    recall_user_shared_facts: 30
    track_full_preference_evolution: 30
    generalizing_to_new_scenarios: 20
    provide_preference_aligned_recommendations: 20
    recalling_the_reasons_behind_previous_updates: 20

global:
  random_seed: 42
  adapters: [persona]
  parallel_workers: 5
  checkpoint_dir: evals/results
  deep_logging: true
```

### Runtime Toggles

- `LONGMEMEVAL_INCLUDE_DATE` (default: `true`) prefixes LongMemEval questions with `(date: ...)`.
- `GRAPHITI_SEARCH_LIMIT` (default: `20`) sets top-k edges/nodes for Graphiti retrieval.
- `GRAPHITI_QUERY_MAX_CHARS` (default: `255`) truncates the retrieval query to match the Zep pipeline.
- `GRAPHITI_INGEST_CONCURRENCY` (default: `1`) controls parallel episode ingestion.
- `GRAPHITI_INGEST_TIMEOUT_S` (default: `600`) caps each Graphiti ingestion call.
- `GRAPHITI_RETRIEVAL_TIMEOUT_S` (default: `0`, disabled) caps Graphiti retrieval.
- `GRAPHITI_PROVIDER` (default: `openai`) selects Graphiti LLM provider (`openai` or `azure`).
- `GRAPHITI_LLM_MODEL`, `GRAPHITI_GENERATOR_MODEL`, `GRAPHITI_RERANKER_MODEL` override Graphiti models.
- `GRAPHITI_RPS` (default: `0`) enables a simple rate limiter for Graphiti LLM calls.
- `AZURE_RERANKER_DEPLOYMENT` / `AZURE_RERANKER_API_VERSION` control the Azure deployment for the Graphiti reranker. If unset, Graphiti defaults to `gpt-4.1-nano` and requires a matching Azure deployment.
- `GRAPHITI_GENERATOR_MAX_TOKENS` (default: `256` on Azure, `0` on OpenAI) caps Graphiti answer length.
- Graphiti detects PersonaMem prompts automatically and enforces letter-only outputs.
- `GRAPHITI_RERANKER_MAX_TOKENS` (default: `32`) sets the reranker completion budget for Azure models.
- `GRAPHITI_RERANKER_TIMEOUT_S` (default: `120`) caps each reranker call to avoid long hangs.

### Creating Custom Configs

```python
from evals.config import EvalConfig, BenchmarkConfig

config = EvalConfig(
    longmemeval=BenchmarkConfig(
        source="evals/data/longmemeval_oracle.json",
        sample_sizes={"multi-session": 20, "temporal-reasoning": 20}
    ),
    random_seed=42,
    adapters=["persona", "mem0"],
    parallel_workers=3
)

config.save("evals/configs/my_custom_config.yaml")
```

---

## Extending the Framework

### Adding a New Memory System

1. **Create adapter** in `evals/adapters/`:

```python
from evals.adapters.base import MemorySystem

class MySystemAdapter(MemorySystem):
    def __init__(self):
        # Initialize your memory system
        pass

    def add_session(self, user_id: str, session_data: str, date: str) -> None:
        """Ingest a single session into the memory system."""
        pass
    
    def add_sessions(self, user_id: str, sessions: List[Dict]) -> None:
        """Bulk ingest sessions. Each dict has 'content' and 'date' keys."""
        for session in sessions:
            self.add_session(user_id, session['content'], session['date'])

    def query(self, user_id: str, question: str) -> str:
        """Retrieve relevant context and generate answer."""
        context = self.retrieve(user_id, question)
        return self.llm.generate(context, question)

    def reset(self, user_id: str) -> None:
        """Clear all memory for a specific user to ensure test isolation."""
        pass
```

2. **Register in CLI** (`evals/cli.py`):

```python
from evals.adapters.mysystem_adapter import MySystemAdapter

ADAPTERS = {
    "persona": PersonaAdapter,
    "mem0": Mem0Adapter,
    "mysystem": MySystemAdapter,  # Add here
}
```

3. **Run evaluation**:

```bash
poetry run python -m evals.cli run \
  --benchmark longmemeval \
  --adapter mysystem \
  --samples 20
```

### Adding a New Benchmark

1. **Create loader** in `evals/loaders/`:

```python
from evals.loaders.unified_loader import UnifiedBenchmarkLoader

class MyBenchmarkLoader:
    def load(self) -> List[Question]:
        # Load your benchmark dataset
        pass

    def stratified_sample(self, sample_sizes: Dict[str, int]) -> List[Question]:
        # Implement sampling logic
        pass
```

2. **Integrate with unified loader**:

```python
# In unified_loader.py
if benchmark == "mybenchmark":
    self.loader = MyBenchmarkLoader(...)
```

3. **Add scoring logic**:

```python
# In runner.py
def evaluate_mybenchmark(question, answer):
    # Your evaluation logic
    pass
```

---

## Project Structure

```
evals/
â”œâ”€â”€ adapters/              # Memory system adapters
â”‚   â”œâ”€â”€ base.py           # Abstract base class
â”‚   â”œâ”€â”€ persona_adapter.py # Persona system
â”‚   â”œâ”€â”€ mem0_adapter.py   # Mem0 system
â”‚   â””â”€â”€ zep_adapter.py    # Zep/Graphiti system
â”‚
â”œâ”€â”€ loaders/              # Data loaders
â”‚   â”œâ”€â”€ longmemeval_loader.py
â”‚   â”œâ”€â”€ personamem_loader.py
â”‚   â””â”€â”€ unified_loader.py
â”‚
â”œâ”€â”€ logging/              # Deep logging infrastructure
â”‚   â”œâ”€â”€ log_schema.py     # Pydantic models
â”‚   â””â”€â”€ deep_logger.py    # Logger utility
â”‚
â”œâ”€â”€ longmemeval/          # LongMemEval evaluation
â”‚   â””â”€â”€ evaluate_qa.py    # Binary judge (GPT-4o)
â”‚
â”œâ”€â”€ configs/              # YAML configurations
â”‚   â”œâ”€â”€ full_eval.yaml
â”‚   â”œâ”€â”€ quick_test.yaml
â”‚   â””â”€â”€ longmemeval_only.yaml
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ longmemeval/
â”‚   â”‚   â””â”€â”€ longmemeval_oracle.json    # 500 questions
â”‚   â”œâ”€â”€ personamem/
â”‚   â”‚   â”œâ”€â”€ questions_32k_32k.json     # 589 questions
â”‚   â”‚   â”œâ”€â”€ questions_128k_32k.json    # 2,727 questions
â”‚   â”‚   â””â”€â”€ questions_1M_32k.json      # 2,674 questions
â”‚   â””â”€â”€ golden_sets/
â”‚       â”œâ”€â”€ longmemeval_golden_set.json      # 220 questions
â”‚       â””â”€â”€ personamem_golden_set.json       # 120 questions
â”‚
â”œâ”€â”€ scripts/              # Utility scripts
â”‚   â”œâ”€â”€ download_personamem.py
â”‚   â”œâ”€â”€ generate_golden_sets.py
â”‚   â””â”€â”€ verify_longmemeval_oracle.py
â”‚
â”œâ”€â”€ results/              # Evaluation results
â”‚   â””â”€â”€ run_YYYYMMDD_HHMMSS/
â”‚       â”œâ”€â”€ deep_logs.jsonl
â”‚       â”œâ”€â”€ summary.json
â”‚       â””â”€â”€ run_metadata.json
â”‚
â”œâ”€â”€ analysis/             # Historical analysis (gitignored)
â”œâ”€â”€ cli.py               # CLI interface
â”œâ”€â”€ config.py            # Config parser
â”œâ”€â”€ runner.py            # Evaluation orchestrator
â””â”€â”€ README.md            # This file
```

---

## Contributing

### Running Tests

```bash
# Test data loaders
poetry run python evals/loaders/personamem_loader.py
poetry run python evals/loaders/longmemeval_loader.py

# Test deep logger
poetry run python evals/logging/deep_logger.py

# Test config parser
poetry run python evals/config.py

# Verify datasets
poetry run python evals/scripts/verify_longmemeval_oracle.py
```

### Code Style

- Follow PEP 8
- Use type hints
- Document public methods
- Keep functions focused and testable

### Submitting Changes

1. Create a feature branch
2. Add tests for new functionality
3. Update documentation
4. Run full evaluation to verify no regressions
5. Submit pull request

---

## Acknowledgments

This framework is built on top of:

- [LongMemEval](https://github.com/xiaowu0162/LongMemEval) (ICLR 2025) - Wu et al.
- [PersonaMem](https://github.com/bowen-upenn/PersonaMem) (COLM 2025) - Shi et al.
- [Mem0](https://github.com/mem0ai/mem0) - Open-source memory layer
- [Zep](https://github.com/getzep/zep) - Long-term memory for LLM apps

### Citation

If you use this evaluation framework in your research, please cite:

```bibtex
@software{persona_eval_framework,
  title = {Persona Memory System - Evaluation Framework},
  author = {InnerNets AI},
  year = {2024},
  url = {https://github.com/saxenauts/persona}
}
```

---

## License

MIT License - See [LICENSE](../LICENSE) for details.

---

## Contact

For questions or feedback:
- GitHub Issues: [saxenauts/persona/issues](https://github.com/saxenauts/persona/issues)
- Email: your-email@example.com

---

**Last Updated**: December 25, 2025
